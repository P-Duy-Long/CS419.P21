# search_app.py

import streamlit as st
import numpy as np
import operator
import sklearn.metrics
import re
import os
from collections import defaultdict

# NLTK imports (ƒê·∫£m b·∫£o ƒë√£ t·∫£i c√°c g√≥i n√†y n·∫øu ch·∫°y l·∫ßn ƒë·∫ßu tr√™n m√¥i tr∆∞·ªùng m·ªõi)
import nltk
try:
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')

from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.stem.porter import PorterStemmer

# Th∆∞ vi·ªán cho BM25 (c·∫ßn c√†i ƒë·∫∑t: pip install rank_bm25)
from rank_bm25 import BM25Okapi

# Th∆∞ vi·ªán cho Semantic Search (c·∫ßn c√†i ƒë·∫∑t: pip install sentence-transformers)
from sentence_transformers import SentenceTransformer

# --- 1. C·∫•u h√¨nh v√† T·∫£i d·ªØ li·ªáu Cranfield ---
# H√†m n√†y s·∫Ω ƒë∆∞·ª£c Streamlit cache ƒë·ªÉ ch·ªâ ch·∫°y m·ªôt l·∫ßn
@st.cache_data
def load_cranfield_data():
    CRAN_DATA_DIR = "./Cranfield/" # Gi·∫£ s·ª≠ b·∫°n ƒë√£ gi·∫£i n√©n Cranfield.zip v√†o c√πng th∆∞ m·ª•c v·ªõi script n√†y, ho·∫∑c c·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
    CRAN_QREL_FILE = os.path.join(CRAN_DATA_DIR, "cran_qrel.txt")
    CRAN_QRY_FILE = os.path.join(CRAN_DATA_DIR, "cran_qry.txt")
    CRAN_DOC_FILE = os.path.join(CRAN_DATA_DIR, "cran_all.txt")

    qrels_dict = defaultdict(list)
    try:
        with open(CRAN_QREL_FILE, 'r') as f:
            for line in f:
                parts = line.strip().split()
                query_id = int(parts[0])
                doc_id = int(parts[1])
                qrels_dict[query_id].append(doc_id)
        # st.write(f"Loaded {len(qrels_dict)} queries with relevance judgments into qrels_dict.")
    except FileNotFoundError:
        st.error(f"Error: {CRAN_QREL_FILE} not found. Please ensure the path '{CRAN_DATA_DIR}' is correct and files exist.")
        st.stop() # D·ª´ng ·ª©ng d·ª•ng n·∫øu kh√¥ng t√¨m th·∫•y file

    documents_raw = []
    try:
        with open(CRAN_DOC_FILE, 'r') as f:
            content = f.read()
            docs_raw = content.split('.I ')[1:]
            for d_raw in docs_raw:
                lines = d_raw.strip().split('\n')
                doc_text = ""
                reading_text = False
                for line in lines[1:]:
                    if line.strip() == '.W':
                        reading_text = True
                        continue
                    if reading_text:
                        if line.startswith('.'):
                            break
                        doc_text += line.strip() + " "
                documents_raw.append(doc_text.strip())
        # st.write(f"Loaded {len(documents_raw)} raw documents.")
    except FileNotFoundError:
        st.error(f"Error: {CRAN_DOC_FILE} not found. Please ensure the path '{CRAN_DATA_DIR}' is correct and files exist.")
        st.stop() # D·ª´ng ·ª©ng d·ª•ng n·∫øu kh√¥ng t√¨m th·∫•y file
    
    N_DOCS = len(documents_raw)
    return documents_raw, N_DOCS, qrels_dict # (qrels_dict kh√¥ng d√πng trong app nh∆∞ng tr·∫£ v·ªÅ cho ƒë·∫ßy ƒë·ªß)

# --- 2. ƒê·ªãnh nghƒ©a c√°c h√†m ti·ªÅn x·ª≠ l√Ω (nh∆∞ b·∫°n ƒë√£ c√≥) ---
Stop_Words = stopwords.words("english")
porter_stemmer = PorterStemmer()

def tokenize(text):
    clean_txt = re.sub('[^a-z\s]+',' ',text)
    clean_txt = re.sub('(\s+)',' ',clean_txt)
    min_length = 3
    words = map(lambda word: word.lower(), word_tokenize(clean_txt))
    words = [word for word in words if word not in Stop_Words]
    words = filter(lambda t: len(t)>=min_length, words)
    tokens = (list(map(lambda token: porter_stemmer.stem(token),words)))
    return tokens

# --- 3. T·∫£i v√† Kh·ªüi t·∫°o M√¥ h√¨nh BM25 (ch·ªâ ch·∫°y m·ªôt l·∫ßn) ---
@st.cache_resource # st.cache_resource d√πng cho c√°c ƒë·ªëi t∆∞·ª£ng ph·ª©c t·∫°p nh∆∞ model
def load_bm25_model(documents):
    st.write("ƒêang kh·ªüi t·∫°o BM25 Model... (Ch·ªâ ch·∫°y l·∫ßn ƒë·∫ßu)")
    corpus_tokenized = [tokenize(doc) for doc in documents]
    bm25_model = BM25Okapi(corpus_tokenized, k1=2.0, b=0.75) # S·ª≠ d·ª•ng tham s·ªë t·ªët nh·∫•t c·ªßa b·∫°n
    st.write("BM25 Model ƒë√£ s·∫µn s√†ng.")
    return bm25_model

# --- 4. ƒê·ªãnh nghƒ©a h√†m get_ranked_documents cho ·ª©ng d·ª•ng web (ch·ªâ d√πng BM25) ---
def get_ranked_documents_for_app(query_text, bm25_model_instance, N_DOCS_val):
    doc_sim = {}
    query_tokens = tokenize(query_text)

    if not query_tokens: # X·ª≠ l√Ω truy v·∫•n r·ªóng sau tokenize
        return []

    bm25_raw_scores = bm25_model_instance.get_scores(query_tokens)
    
    # T·∫°o dictionary ƒëi·ªÉm s·ªë
    for item in range(N_DOCS_val):
        doc_sim[item + 1] = bm25_raw_scores[item]

    # L·ªçc b·ªè c√°c t√†i li·ªáu c√≥ ƒëi·ªÉm s·ªë 0 ho·∫∑c r·∫•t nh·ªè v√† s·∫Øp x·∫øp
    ranked_docs = sorted([(doc_id, score) for doc_id, score in doc_sim.items() if score > 1e-6],
                         key=operator.itemgetter(1), reverse=True)
    return ranked_docs

# ==============================================================================
# B·∫ÆT ƒê·∫¶U ·ª®NG D·ª§NG STREAMLIT
# ==============================================================================

st.set_page_config(page_title="Cranfield Search Engine (BM25)", layout="centered")

st.title("üîé Cranfield Search Engine")
st.markdown("M·ªôt c√¥ng c·ª• t√¨m ki·∫øm ƒë∆°n gi·∫£n cho b·ªô d·ªØ li·ªáu Cranfield.")

# T·∫£i d·ªØ li·ªáu v√† m√¥ h√¨nh m·ªôt l·∫ßn
documents_raw, N_DOCS, qrels_dict = load_cranfield_data()
bm25_model_instance = load_bm25_model(documents_raw)


# √î nh·∫≠p truy v·∫•n
query = st.text_input("Nh·∫≠p truy v·∫•n c·ªßa b·∫°n v√†o ƒë√¢y:", placeholder="e.g., aerodynamic characteristics of wings")

# N√∫t t√¨m ki·∫øm
if st.button("T√¨m ki·∫øm"):
    if query:
        st.subheader(f"K·∫øt qu·∫£ cho: \"{query}\"")
        
        # Th·ª±c hi·ªán t√¨m ki·∫øm
        results = get_ranked_documents_for_app(query, bm25_model_instance, N_DOCS)
        
        if results:
            st.write(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£. Hi·ªÉn th·ªã 10 k·∫øt qu·∫£ h√†ng ƒë·∫ßu.")
            for i, (doc_id, score) in enumerate(results[:10]):
                if 0 < doc_id <= len(documents_raw):
                    doc_content = documents_raw[doc_id - 1]
                    snippet = doc_content[:400] + "..." if len(doc_content) > 400 else doc_content
                    
                    st.markdown(f"---")
                    st.write(f"**H·∫°ng {i+1}: T√†i li·ªáu ID {doc_id} (ƒêi·ªÉm s·ªë: {score:.4f})**")
                    st.markdown(f"```\n{snippet}\n```") # Hi·ªÉn th·ªã snippet trong kh·ªëi code
                else:
                    st.warning(f"H·∫°ng {i+1}: T√†i li·ªáu ID {doc_id} (ƒêi·ªÉm s·ªë: {score:.4f}) - Kh√¥ng t√¨m th·∫•y n·ªôi dung t√†i li·ªáu g·ªëc.")
        else:
            st.info("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o kh·ªõp v·ªõi truy v·∫•n c·ªßa b·∫°n.")
    else:
        st.warning("Vui l√≤ng nh·∫≠p truy v·∫•n ƒë·ªÉ t√¨m ki·∫øm.")

st.markdown("---")
st.write("ƒê·ªì √°n m√¥n h·ªçc H·ªá th·ªëng truy xu·∫•t th√¥ng tin")